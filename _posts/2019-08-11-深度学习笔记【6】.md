---
layout:     post                    # 使用的布局（不需要改）
title:      深度学习笔记【6】              # 标题 
subtitle:   神经网络训练【Batch Normalization】 #副标题
date:       2019-08-11            # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 神经网络
---

# Six

最近拜读了谷歌在2015年发表的一篇论文[Batch Normalization](https://arxiv.org/abs/1502.03167),正是这篇论文，给深度学习带来了又一个强大的方法。



## BN方法提出的背景

深度学习中广泛使用的训练方法大多与梯度有关，如SGD、Adam等，就从最简单的SGD开始举例。



不妨设我们需要训练的神经网络中的参数为$\Theta$  ,则神经网络训练所需要做的事情也就是：


$$
\Theta=\arg \min _{\Theta} \frac{1}{N} \sum_{i=1}^{N} \ell\left(\mathrm{x}_{i}, \Theta\right)
$$


这其中的$x_{1\dots N}$都是训练集中的数据。



如果我们使用的方法是SGD(随机梯度下降)，那么我们并不是一次性训练所有的数据，而是分批次训练小样本数据，也就是所谓的`mini-batch`,采用小样本训练而不是整体训练会带来很多好处，比如加快训练速度等。但是需要注意的是，在实际训练中我们常常需要对神经网络的超参数进行很小心的设定，由此来提高训练速度和训练精度。



在谷歌的论文中，提到了这样的现象：

> While stochastic gradient is simple and effective, it requires careful tuning of the model hyperparameters,specifically the learning rate used in optimization, as well as the initial values for the model parameters. The training is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers – so that small changes to the network parameters amplify as the network becomes deeper.

形象的来看，由于我们采用的是小样本训练，带来这样的问题：同样的一层神经元，在接受不同的batch的输入时，有很大可能接受到的样本的分布是具有不小的区别的，这也就造成了训练的时候我们的神经网络需要不断地接受不同分布的样本来训练，这样显然会给我们的训练增加难度。



考虑一个简单的例子，下面是一个神经网络计算损失函数的情况：


$$
\ell=F_{2}\left(F_{1}\left(\mathrm{u}, \Theta_{1}\right), \Theta_{2}\right)
$$


其中，$F_1,F_2$是任意的变换函数，$u$是神经网络的输入，$\Theta_1,\Theta_2$分别是两层网络的参数。



我们的目的是通过学习到参数$\Theta$来使得$\ell$最小化。



对于第二层$F_2$，我们可以将第一层的输出作为第二层的输入，也就是$x=F_1(u,\Theta_1)$,然后有：


$$
\ell=F_2(x,\Theta_2)
$$


下面是梯度下降的一步：


$$
\Theta_{2} \leftarrow \Theta_{2}-\frac{\alpha}{m} \sum_{i=1}^{m} \frac{\partial F_{2}\left(\mathrm{x}_{i}, \Theta_{2}\right)}{\partial \Theta_{2}}
$$


其中的m是batch的大小，$\alpha$是学习的步长。



我们不妨设想一个情况，每一次的输入都是不同的分布，也就意味着我的参数在每一次都需要适应新的分布，这样的训练显然不会太有效率。



## Internal Covariate Shift

上面的例子仅仅是两层，可想见，如果神经网络变得很深，那么结果许多层神经网络的变换，原本的输入样本之间的微小间隔很有可能会变得十分剧烈，可以看成是神经网络中的**蝴蝶效应**。



这样带来的结果就是神经网络很难适应变化剧烈异常的输入，导致难以收敛，训练效率低下。



谷歌在论文中提到：

>We define **Internal Covariate Shift** as the change in the distribution of network activations due to the change in network parameters during training



我们不妨设想，如果每一层的输入的分布都是固定的，那么可以想见，这种剧烈变化分布的情况就会大大减小。

也就是：

> we would like to ensure that, for any parameter values, **the network always produces activations with the desired distribution.**



而我们想要解决**ICS**这样的问题，这就引出了**Batch Normalization**这样的trick。



## Normalization via Mini-Batch Statistics

由于深度神经网络在训练过程中的各层输入变化常常十分剧烈，为了减缓这样的情况，我们需要找个方法来管一管，这个方法我们称为**Normalization**, 也就是正规化处理。



对于神经网络中的某一层(维度为d)，并且这一层的输入为$\mathrm{x}=\left(x^{(1)} \ldots x^{(d)}\right)$,我们对输入的每一个维度都进行下面的操作：


$$
\widehat{x}^{(k)}=\frac{x^{(k)}-\mathrm{E}\left[x^{(k)}\right]}{\sqrt{\operatorname{Var}\left[x^{(k)}\right]}}
$$


![图示](https://ae01.alicdn.com/kf/Hdf17b2b556104fd0819b305685e75b8ft.png)

有人可能会问了，像我这样改变神经元输入的分布，不是会造成神经元辛苦工作*白做*吗？



不！**神经元的工作永远不会白做，我们尊重底层劳动人民的辛勤劳作！**



为此，我们引入又一个仿射变换(affine),也就是：


$$
y^{(k)}=\gamma^{(k)} \widehat{x}^{(k)}+\beta^{(k)}
$$


其中的$\gamma ,\beta$都是我们需要训练的参数。



这个变换的作用是什么呢？



谷歌是这么解释的：

>Note that simply normalization each input of a layer may change what the layer can represent. For insance, normalization the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. To address this, we make sure that *the transformation inserted in the network can represent the **identity transform.***



简单的说，我们要确保我们的操作万无一失，哪怕这样的操作会给我们的训练带来坏处，我们也可以通过后续的变换将其修正过来，也就是我们取：


$$
\gamma^{(k)}=\sqrt{\operatorname{Var}\left[x^{(k)}\right]}\\
\beta^{(k)}=\mathrm{E}\left[x^{(k)}\right]
$$


就可以将之前的Normalization操作恢复。



可能会有人觉得这样的行为就像脱裤子放屁，但是，这一个操作大大提高了模型的**capacity**,并且还使得训练更容易收敛。 



![](https://ae01.alicdn.com/kf/Hbc2a4c0a09cd4dbbb8951e4e262087ddJ.png)



上图便是**Batch Normalization**的流程。



### Batch Normalization的参数训练



既然我们通过*Batch Normalization*的操作引入了新的参数，那么我们就需要在神经网络的训练过程中对引入的新的参数进行训练。



训练方法不会发生变化，还是使用**BP**算法，在那之前，我们先使用**chain rule**来计算出各个参数的梯度。



下面的所有计算，都建立在$\frac{\partial \ell}{\partial y_i} $是已知的基础上。



首先，


$$
y=\gamma \widehat{x}+\beta
$$


那么也就是说：


$$
\frac{\partial \ell}{\partial \widehat{x}_{i}}=\frac{\partial \ell}{\partial y_{i}} \cdot \gamma
$$

下面我们考察对$\sigma_{\mathcal{B}}$的梯度，由于：
$$
\widehat{x}_{i} =\frac{x_{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}
$$


这就表示$\sigma_{\mathcal{B}}$对$\widehat{x}_{}$ 的每一个维度的元素都有影响，那么由此：


$$
\frac{\partial \ell}{\partial \sigma_{\mathcal{B}}^{2}}=\sum_{i=1}^{m} \frac{\partial \ell}{\partial \widehat{x}_{i}} \cdot\left(x_{i}-\mu_{\mathcal{B}}\right) \cdot \frac{-1}{2}\left(\sigma_{\mathcal{B}}^{2}+\epsilon\right)^{-3 / 2}
$$

接下来考虑对$\mu_{\mathcal{B}}$的梯度，由于$\mu_{\mathcal{B}}$对$\widehat{x}$和$\sigma_{\mathcal{B}}^2$具有影响，故此有：



$$
\begin{equation}
\frac{\partial \ell}{\partial \mu_{\mathcal{B}}}=\left(\sum_{i=1}^{m} \frac{\partial \ell}{\partial\widehat{x}_{i}}
\cdot \frac{-1}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}\right)+\frac{\partial \ell}{\partial \sigma_{\mathcal{B}}^{2}} \cdot \frac{\sum_{i=1}^{m}-2\left(x_{i}-\mu_{\mathcal{B}}\right)}{m}
\end{equation}
$$


下面类似的可以得到：



$$
\begin{aligned} \frac{\partial \ell}{\partial x_{i}} &=\frac{\partial \ell}{\partial \widehat{x}_{i}} \cdot \frac{1}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}+\frac{\partial \ell}{\partial \sigma_{\mathcal{B}}^{2}} \cdot \frac{2\left(x_{i}-\mu_{\mathcal{B}}\right)}{m}+\frac{\partial \ell}{\partial \mu_{\mathcal{B}}} \cdot \frac{1}{m} \\ \frac{\partial \ell}{\partial \gamma} &=\sum_{i=1}^{m} \frac{\partial \ell}{\partial y_{i}} \cdot \widehat{x}_{i} \\ \frac{\partial \ell}{\partial \beta} &=\sum_{i=1}^{m} \frac{\partial \ell}{\partial y_{i}} \end{aligned}
$$


由此我们可以看出，**Batch Normalization**这个操作是可微的。



### Inference

训练完成，就到了使用模型的阶段了，我们称为**Inference**(推断)。



在训练阶段时，我们往往可以指定Batch的size，而到了推断的阶段，一般我们是无法指定输入的规模的，可能是一个，也可能是很多个。所以在推断的阶段，BN算法就不能像是在训练阶段一样操作了。



谷歌在论文中是这么说的：



> The normalization of activations that depends on the mini-batch allows effcient training, but is neither necessary nor desireable during inference, we want the output to depend only on the input, deterministically.For this, once the network has been trained, we use the normalization
> $$
> \widehat{x}=\frac{x-\mathrm{E}[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}
> $$
> using the population, rather than mini-batch, statistics.



也就是在推断的时候，我们希望输出仅仅取决于输入，并且采用的$E[x],Var[x]$都不是简单的输入样本的均值和方差，而是总体(population)的。



值得一提的是，这里的方差采用的是无偏的方差，也就是：


$$
\operatorname{Var}[x]=\frac{m}{m-1}\cdot E_{\mathcal{B}}[\sigma_{\mathcal{B}}^2]
$$
下图是算法的流程。



![](https://ae01.alicdn.com/kf/H3f782ab26e2f4b9d9462b4d795479a52Q.png)

### Convolutional Networks

上面的例子都是基于全连接神经网络(FNN)的,下面我们介绍一些卷积神经网络的相关BN操作。



谷歌提到：

> For convolutional layers, we additionally want the normalization to obey the convolutional property — so that different elements of the same feature map, at different locations, are normalized in the same way.

所以为了不破坏卷积层的性质，我们的BN算法流程在卷积情况下需要进行调整，简单来说就是：

- 根据输入的batch size，选取多个样本，作为BN的对象，这些样本都使用同一个$\gamma ,\beta$.
- 如果使用多个卷积核，在这样的情况下就有多个$\gamma,\beta$需要学习



### Batch Normalization的优点

#### 可以使用更大的学习步长

在使用BN前，我们在选取步长的时候都会十分小心，因为步长大一点就会带来训练过程震荡、难以收敛等问题。而现在，使用BN后，我们完全可以选取更大的步长，原因如下：



对于常量$a$,
$$
\mathrm{BN}(W \mathrm{u})=\mathrm{BN}((a W) \mathrm{u})
$$
这也就意味着：


$$
\begin{aligned} \frac{\partial \mathrm{BN}((a W) \mathrm{u})}{\partial \mathrm{u}} &=\frac{\partial \mathrm{BN}(W \mathrm{u})}{\partial \mathrm{u}} \\ \frac{\partial \mathrm{BN}((a W) \mathrm{u})}{\partial(a W)} &=\frac{1}{a} \cdot \frac{\partial \mathrm{BN}(W \mathrm{u})}{\partial W} \end{aligned}
$$


也就是说，我们选取了较大的步长，使得参数增加的很大，但是并不会影响其对下一层的输入，因为经过了**Normalization**这一步骤。



谷歌如此形容：

> The scale does not affect the layer Jacobian nor, consequently, the gradient propagation. Moreover, larger weights lead to smaller gradients, and Batch Normalization will stabilize the parameter growth.



#### 可以正则化模型

也就是说，使用了BN，我们就不需要(或者说不太需要)，在误差函数中增加正则项，也不(太)需要使用**Dropout**等操作来规避过拟合情况。



## 总结

BN是个好东西，可以很大程度上加快训练速度。