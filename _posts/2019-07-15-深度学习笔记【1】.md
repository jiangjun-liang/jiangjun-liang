---
layout:     post                    # 使用的布局（不需要改）
title:      深度学习笔记【1】               # 标题 
subtitle:   Hello World!-手写数字识别 #副标题
date:       2019-07-15           # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 神经网络
---
# One

学习程序设计语言，我们最先实现的程序，大多是“Hello World”程序，而在深度学习里，手写数字识别，也相当于“Hello World”.

本文从两个角度实现手写数字识别，分别用**numpy**单独实现，以及使用**tensorflow**框架实现，数据来源于tensorflow自带的MNIST数据集。

这个启发来自于复旦大学教授[邱锡鹏](https://xpqiu.github.io/)教授的GitHub上的[课程练习](<https://github.com/nndl/exercise>)

## numpy实现朴素神经网络

首先介绍一下[**交叉熵**](<https://en.wikipedia.org/wiki/Cross_entropy>)的概念

### 交叉熵

在物理学中，熵用来表示一个系统的**复杂程度**，而在信息学领域，熵多用于表示**不确定性**。



举个例子：

| 巴西队踢进世界杯 | 中国队踢进世界杯 |
| :--------------: | :--------------: |
|      P=0.9       |      P=0.01      |

这里有两个事件，分别具有不同的概率，*巴西队踢进世界杯*的概率为0.9，这表示的信息就不如*中国队踢进世界杯*带来的多，因为中国队踢进世界杯的概率十分的小，一旦这样的事件发生了，往往蕴含着很多的信息。



在机器学习领域，我们常常使用交叉熵，来作为损失函数，用于衡量我们的模型是否足够好。



用P来表示样本的真实分布，用Q来表示我们的预测分布。



如，某一个样本的分布为：[0;1;0], 而我们预测的分布为：[0.3;0.4;0.3],那么我们预测的这个分布的交叉熵就可以写为：


$$
L=-(0*log(0.3)+1*log(0.4)+0*log(0.3))=0.9163
$$


而如果我们预测的分布变为：[0.05;0.9;0.05] , 那么交叉熵就写为：


$$
L=-(0*log(0.05)+1*log(0.9)+0*log(0.05))=0.1054
$$


看得出，使用后一种分布可以得到更小的误差，因为我们预测的十分有把握，而前一种分布更多的有一种瞎猜的成分。



一般的，交叉熵写作：


$$
L=-\sum p_ilog(q_i)
$$


### Softmax函数

接着我们介绍一个在机器学习中使用很广泛的函数，[**Softmax**函数]([https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0](https://zh.wikipedia.org/wiki/Softmax函数))。



如果我有一个输入$x,x\in R^n$



我想要根据这个输入，得到同样属于$R^n$的一个输出向量，其中的每一个分量都表示这一种分类的预测概率。


$$
\sigma(x)_i=\frac{e^{x_i}}{\sum_j e^{x_j}}
$$


显然，这样定义的函数具有输出的向量，所有分量之和为1的性质，并且每一项都非负，这就可以很好的表示概率。



并且如果我们对$\sigma(x)_i=\frac{e^{x_i}}{\sum_j e^{x_j}}$ 关于$x_i$求偏导，可以得到：


$$
\frac{\partial \sigma_i}{\partial x_i}=\sigma_i(1-\sigma_i)
$$


这个性质非常好，我们可以通过这个性质，很方便的求出导数，用于训练模型。



### 实现

为了将代码写的更加便于理解，我这里在训练过程中，采用了一个样本训练一次的方法，这样带来的后果就是，训练十分的缓慢，下面的代码跑了一晚上才训练结束，正确率在**85%**左右。

```python
# -*- coding: utf-8 -*-

import numpy as np
from tensorflow.examples.tutorials.mnist import input_data

class Model():
    def __init__(self):
        mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
        self.n_in = 784  # 每张图片是 28 *28 像素
        self.n_out = 10  # 总共有 10个类
        self.max_epochs = 10000  # 最大训练步数 1000步
        self.Weights = np.random.rand(self.n_in,self.n_out) # initialize W 0
        self.biases = np.zeros(self.n_out)  # initialize bias 0
        for i in range(self.max_epochs):
            batch_xs, batch_ys = mnist.train.next_batch(100)
            batch_xs = np.array(batch_xs)
            batch_ys = np.array(batch_ys)

            self.train(batch_xs, batch_ys, 0.0001)
            if i % 5 == 0: 
                accuracy_test = self.compute_accuracy(np.array(mnist.test.images[:500]), np.array(mnist.test.labels[:500]))
                print("#"*30)
                print("compute_accuracy:",accuracy_test)
                print("cross_entropy:",self.cross_entropy(batch_ys,self.output(batch_xs) )) # 输出交叉熵损失函数
    def train(self,batch_x,batch_y, learning_rate):# 训练数据 更新权重
        #在下面补全（注意对齐空格）
        for x,y in zip(batch_x,batch_y):
            H=np.add(np.dot(x,self.Weights),self.biases)
            S=np.exp(H).sum()
            H=np.exp(H)/S#1x10的向量，第i个位置的数值表示预测为第i种类别的概率
            #对w_ij开始进行更新
            for i in range(0,10):
                for j in range(0,748):
                    self.Weights[j][i]=self.Weights[j][i]+learning_rate*(x[j]*y[i]*(1-H[i]))
                self.biases[i]=self.biases[i]+learning_rate*(y[i]*(1-H[i]))          
        return
    def output(self, batch_x):# 输出预测值
        # 注意防止 上溢出和下溢出
        def softmax(x):
            e_x = np.exp(x-np.max(x))
            return e_x / (e_x.sum(axis=0)) +1e-30  #
        prediction = np.add(np.dot(batch_x, self.Weights),self.biases)
        result =[]
        for i in range(len(prediction)):
            result.append(softmax(prediction[i]))
        return np.array(result)
        
    def cross_entropy(self,batch_y,prediction_y): #交叉熵函数
        cross_entropy = - np.mean(
            np.sum(batch_y * np.log(prediction_y),axis=1))
        return cross_entropy
        
    def compute_accuracy(self,xs, ys):# 计算预测精度
        pre_y = self.output(xs)
        pre_y_index = np.argmax(pre_y,axis =1)
        y_index = np.argmax(ys,axis =1)
        count_equal = np.equal(y_index,pre_y_index)
        count = np.sum([1 for e in count_equal if e ])
        sum_count =len(xs)
        return  count * 1.0 / sum_count
Model()
```



## Tensorflow实现

使用**Tensorflow**来实现的好处就在于，不必每次都手动求导，并且运行的的速度也会有很大的提高。



点击[这个](<https://colab.research.google.com/drive/1SgFQuY1huYe4Zr8_DNC0GyM0TxvUPLT6>),在线运行下面的代码。



```python
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
```

```python
import tensorflow as tf
```

### 模型的建立

```python
 x = tf.placeholder("float", [None, 784])
#x的用途是作为输入，【None,784】表示一个二维的浮点数形式的张量，None表示不限制输入的个数，784是单个训练样本(28*28)转化为一维向量的维度数
W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))
#W表示的是权重，b表示的是偏置值，其中W是784×10的矩阵，输入是1×784的向量x，这样Wx+b得到的就是1×10的向量，对应0~9十个数字
```


```python
y = tf.nn.softmax(tf.matmul(x,W) + b)
#y表示输出的预测值，这里采用的hypothesis函数是softmax函数
```

### 模型的训练


```python
y_ = tf.placeholder("float", [None,10])
#首先先给正确值预留一个位置，这里的类型同x一样，这是为了与我们得到的预测值进行比较，方便接下来的训练

```


```python
cross_entropy = -tf.reduce_sum(y_*tf.log(y))
#交叉熵，可以用于评价模型的确定程度，与物理学中的熵定义相同
```


```python
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)#定义训练的一步
init = tf.global_variables_initializer()#初始化之前设置的变量
sess = tf.Session()#设置一个Session来开始模型的训练
sess.run(init)#启动初始化
for i in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)#使用mnist的训练集作为数据进行训练，每次选取100组x与y_
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})#把选取的数据送去训练

```

###  模型的评价

```python
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))

```


```python
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
```


```python
sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})
#准确率约为91%
```

