---
layout:     post                    # 使用的布局（不需要改）
title:      深度学习笔记【7】              # 标题 
subtitle:   神经网络训练【Dropout】 #副标题
date:       2019-10-05          # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 神经网络

---



# Seven

许久没有写深度学习的博客了,不仅仅是因为懒,前一阵子在忙数学建模,答应朋友的事情还是要认真做的。



这一次让我们把求知的目光指向**Dropout**,这个出现在2014年的论文，并且几乎成为神经网络“标配”的trick。



本文是在拜读完Hiton等人与2014年发布的论文“[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) ”后写就的。有兴趣的同学可以去读原文，原文写得相当好，远不是我能及的。



## 背景

深度神经网络由于其强大的学习能力,而备受瞩目,其所具有的多层非线性Hidden layers 让它们可以从数据集中学到很复杂的模型。



但是！



由于我们不可能将所有的可能出现的数据都送入神经网络的训练中,所以深度神经网络普遍存在**Overfitting**的现象。计算机科学家也尝试过使用一些正则化的手段来减少Overfitting的现象,比如: **L1;L2 Normalization**,以及其他的一些手段。



换个角度思考,不考虑计算成本,我们最优的模型应该是什么样的？



给定一个模型的结构，在枚举其所有可能的参数变化的模型上，依照其输出结果的准确度给其加权平均，这样就可以得到相当好的模型。(Xiong et al.,2011;Salakhutdinov and Mnih,2008)



这样的方法虽然理论上可以得到相当好的模型，但是却不能在实际中使用，因为成本太高。 



上面的方法就好像有一个具有相当多人口的国家，在这个国家里的每一件事都需要大家一起投票(相当民主),但是这里的话语权却不一样。大家按照每个人在训练集上的单独决策结果来给每个人分等级，决策结果好的，等级高，反之等级低。最后的话语权就按照每个人的等级来分配，如果一个人的等级相当低，可能他的投票并不太准确，这个人就不善于领导，投了也白头，他的票数就可以按 1:0.0001 来折算，反之，如果一个人的等级相当高，那么他的票数就很有可能一票顶十票。



虽然这样的方法十分民主，而且最后的结果也很不错，但是如果让一个人口超级多的国家按照这样的模式来管理，那么举办投票、统计票数、等用于决策的成本就会非常高。这样反而得不到想要的结果，这样的模式虽然**精准**但不够**高效**。



后来这个国家转变了思路，开始借鉴中国，采用人民代表大会制度，每年召开一次人民代表大会，由来自不同身份的人民代表(不同架构的模型)投票决策。由于人民代表的数目要远远小于人民的数目，这样的模式就**比较高效**,同时也**比较精确**。



但是在一些很急的事情上，我们不可能要等所有人民代表聚集起来开会才决定，也就是说，做出反应的速度要**快**。



翻译回深度学习的语言，我们希望用**一个神经网络就能达到多个神经网络加权平均的精度**。



> Dropout is a technique that addresses both these issues.



## 什么是Dropout

> Dropout is a technique that addresses both these issues.**It prevents overfitting and provides a way of approximately combining exponentially many different neural network architectures efficiently**.The term “dropout” refers to dropping out units(hidden and visible) in a neural network.By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections.



上面这段话是论文中的解读，也就是说，Dropout就是在训练神经网络的过程中，暂时地丢掉一些神经元以及它们的输入输出，用下面这幅图来表示。



![](https://ae01.alicdn.com/kf/H65eb6612e874488fb4388e77a72d55a3I.jpg)



上图中的dropout只是暂时的，在训练的时候会暂时的丢掉(里面的数据并不会)。



采用dropout的方法来训练神经网络，其实就相当于在一个比原本的神经网络更**瘦**的神经网络集合上采样，根据排列组合的知识，对于一层具有n个神经元的layer来说，采样具有$2^n$种可能，这些神经网络是高度共享参数的。



> So training a neural network with dropout can be seen as **training a collection of $2^n$
> thinned networks with extensive weight sharing**, where each thinned network gets trained
> very rarely, if at all.



而在测试的时候，我们就不能继续drop原本的那些神经元了，因为如果还是drop那些神经元，我们就需要在$2^n$的量级上做平均。这是十分耗费计算资源的，我们采取的是一种很巧妙的方法。



![](https://ae01.alicdn.com/kf/H9a57c981086a4f6081c87993dbba7f79G.jpg)



只需要在每个神经元的输出上乘一个drop的概率 $p$ 就好了，这样就可以得到很好的average的效果。



下面这段话是论文中夸耀自己的方法有多么好的:



> We found that training a network with dropout and using this approximate averaging method at test time leads to **significantly lower generalization error** on a wide variety of classification problems compared to training with other regularization methods.



虽然有些王婆卖瓜的嫌疑，但是说的几乎都是实话。



## 为什么Dropout是有效的

### Motivation

在论文中，作者给出了一个有关于生物学的intuition(可见计算机科学家不仅仅需要会本领域的知识,了解其他领域的知识是多么的重要！)。



但凡上过中学的同学对于有性生殖都不陌生,哪怕你现在已经记不清腺嘌呤、胞嘧啶、鸟嘌呤、胸腺嘧啶……



但是你一定忘不了初次了解有性生殖后的那种恍然大悟，那种通透的感觉。



有性生殖在自然界是十分普遍的，而处于食物链高层的生物基本都是有性生殖的。



有性生殖大概可以这样形容，携带遗传基因的两性分别贡献一组染色体，这两组染色体再在父辈的基础上产生些许的变异，这就得到了一个新的个体的遗传密码。



与之对应的无性生殖就无趣的多，只是将父辈的基因遗传给子辈，最多在其基础上产生些许的变异。



我们知道的一个事实是，真实世界是十分复杂的，一个生物需要面对许多复杂的情况，在这样的环境下，一个生物的基因之间的相互作用可能是十分复杂的，相互之间的协同也应该很紧密，以次来应对各种可能出现的突发状况。



那么，我们如果简单的这样想：由于父辈的基因之间的相互作用关系就已经十分复杂，那么对其做大幅度的修改就很有可能破坏这样的相互作用，是否会带来子辈在表现性状上的孱弱？而无性繁殖不存在这样大幅度的修改，应该是更加有优势呀。



然而，有性繁殖却占据主导地位。



论文中对此给出了一种可能的解释：



> One possible explanation for the superiority of sexual reproduction is that, over the long
> term, the criterion for natural selection may not be individual fitness but rather mix-ability of genes. The ability of a set of genes to be able to work well with another random set of genes makes them more robust.



既然基因总是不能依赖于大多数基因们的合作来表现性状，其自身就必须要可以适应多数的环境。



打个比方，一支军队，如果所有士兵都能很好的相互配合，但是一旦缺少某一部分士兵(比如炊事兵)，这支军队就几乎丧失战斗力(因为队伍之中对炊事兵过于依赖，导致最简单的伙食都无法自己做)，并不能够适应所有的突发情况。如果这个时候我们有一支军队，队伍中的所有士兵几乎都是全能选手，这样的军队，不仅战斗力不会弱，而且可以适应几乎所有的突发情况。



上面提到的两种军队，就可以看成无性生殖和有性生殖之间的类比。



另外，我在[这篇博文](https://blog.csdn.net/oBrightLamp/article/details/84105097)中看到了一个十分形象的比喻:

> 假设某公司存在一个职能稳定, 合理分工团队.
>
> 因为某些不可避免的原因, 该团队的成员每天都有 50% 概率不能参与工作.
>
> 为了完成任务, 需要其他同事**加班完成**缺席员工任务.
>
> 一段时间后, 该团队的成员普遍学会了其他同事的相关工作技能.
>
> 于是, 该团队拥有了更好的泛化能力.

读完不仅恍然大悟，更叫人泪如雨下。



同样的，一个神经网络，如果采用了dropout的过程，就可以提高单个神经元的战斗力与适应环境的能力，因为每一次的drop，你都不知道哪些神经元被drop掉了，所以每一次的神经元都与不同的神经元进行配合，这样训练出来的神经网络会有更好的鲁棒性，也就可以减少overfitting的现象。



## 如何使用Dropout

### feed-forward



首先我们对一般的神经网络做描述。



考虑一个具有$L$层hidden layer的神经网络，让$l \in \{1,2,\dots,L\}$为每一层的index，记$z^{(l)}$作为输入第$l$层的向量，$y^{(l)}$记为第$l$层的输出，($y^{(0)}=x$ 是神经网络的输入),$W^{(l)},b^{(l)}$分别作为第$l$层的权重和偏置，feed-forward过程可以如下描述:



$$
\begin{aligned} z_{i}^{(l+1)} &=\mathbf{w}_{i}^{(l+1)} \mathbf{y}^{l}+b_{i}^{(l+1)} \\ y_{i}^{(l+1)} &=f\left(z_{i}^{(l+1)}\right) \end{aligned}
$$


其中的$f$为激励函数。



![](https://ae01.alicdn.com/kf/H6e606dbe11c9434e886a6257e9736b6eR.jpg)



而加上dropout后，feed-forward过程就可以写成:


$$
\begin{aligned} r_{j}^{(l)} & \sim \text { Bernoulli }(p) \\ \widetilde{\mathbf{y}}^{(l)} &=\mathbf{r}^{(l)} * \mathbf{y}^{(l)} \\ z_{i}^{(l+1)} &=\mathbf{w}_{i}^{(l+1)} \widetilde{\mathbf{y}}^{l}+b_{i}^{(l+1)} \\ y_{i}^{(l+1)} &=f\left(z_{i}^{(l+1)}\right) \end{aligned}
$$


也就是说，先生成一个服从参数为$p$的伯努利分布的向量，接着让输入这一层的向量做相应位置的乘法，本质上就是选取一些神经元，使得其的输入为0，接下来的操作就和标准的神经网络没什么区别了。

![](https://ae01.alicdn.com/kf/He5b775d4663a47b2bd0b0816e2e1c002F.jpg)



### Backpropagation

从上面的feed-forward就能看出，虽然dropout让网络的结构变得怪怪的，但是从数学上看，并不复杂。



设loss function为$E$,现在我想要求其关于第$i$层的$y^{(l)}$的梯度，根据链式法则，有:




$$
\frac{\partial E}{\partial y^{(l)}}=\frac{\partial E}{\partial\widetilde{\mathbf{y}}^{(l)}}\frac{\partial \widetilde{\mathbf{y}}^{(l)}}{\partial y^{(l)}}
$$


根据$\widetilde{\mathbf{y}}^{(l)} $的定义我们可以得到：


$$
\frac{\partial \widetilde{\mathbf{y}}^{(l)}_j}{\partial y^{(l)}_j}=r^{(l)}_j
$$


也即是：


$$
\frac{\partial E}{\partial y^{(l)}}=\frac{\partial E}{\partial\widetilde{\mathbf{y}}^{(l)}}\odot r^{(l)}_j
$$


其余部分的梯度也不难算出，在这里就不算了，有兴趣的读者自己可以算着解闷。



## 什么时候应该使用Dropout

并不是所有情况都需要使用dropout，dropout的初衷是为了解决过拟合问题，只有当训练集的结果不错，而测试集的结果很差的时候，我们才需要解决过拟合的问题。



如果说本身这个模型就已经出现了underfitting的现象，dropout是没有用的。



虽然目前的计算机的计算能力都很强，训练的模型都具有不少参数，过拟合现象普遍存在，但是dropout技术并不是灵丹妙药，更加细心的做特征提取、结构优化所带来的收益可能更佳。