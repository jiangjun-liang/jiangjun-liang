---
layout:     post                    # 使用的布局（不需要改）
title:      深度学习杂谈【1】               # 标题 
subtitle:   为什么说深度学习的优化是nonconvex的 #副标题
date:       2019-07-19            # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 神经网络

---

# 什么是Convex

据说很久以前，人们将优化问题分为两种：

- 线性问题
- 非线性问题

而线性问题在数学家们的努力下，有了十分完善的理论，目前几乎所有的线性问题，都可以得到很好的解决，也就是可以找到所谓的全局最优解。



而非线性问题，在相当长的一段时间都是十分难以解决的。后来，人们发现，在非线性问题中，存在一类特殊的问题，它是可以像线性问题一样得到很好的解决的，这样的问题都有一个很显著的特征，那就是**Convex**,也就是凸优化问题。



什么样的问题是凸优化呢？



![](https://ae01.alicdn.com/kf/HTB1.wzWaLb2gK0jSZK9761EgFXaf.png)



如果说一个函数是凸函数，从几何的角度来说，就是从该函数的曲线上任意两点，所得到的线段都是在函数的曲线的上方的，当然，宽泛的讲，线性的函数也是凸函数。



凸函数有什么优点呢？



不难看出，凸函数在可行域里不存在局部最小值。



这就给我们使用和梯度相关的算法带来了便利，因为梯度相关的算法常常会有梯度为0，算法终止，而还没有找到全局最优解的情况。



所以说，如果一个问题可以归结到凸优化的范畴，那么这个问题就可以很漂亮的解决。



## 为什么说深度学习的问题是nonconvex的？

深度学习中，有一步非常重要的步骤就是**训练**,而训练，也就是调整神经网络的参数，使得这个神经网络的损失函数最小化，从这个角度来说，深度学习离不开优化。



为什么我们可以说深度学习是非凸优化问题呢？



来看一幅图：



![](https://cy-pic.kuaizhan.com/g3/f2/49/7eea-b05e-4c6d-bb59-24823ba3ab9c40)



这幅图什么意思？



我们经过训练，得到了一组参数(由神经元和边的颜色体现参数的大小)，但是如果改变神经元的排列，对下一层神经元的输入没有任何区别。



这意味着什么？这表示哪怕我们找到了一组当前梯度为0的最优解，但是也可以经过简单的排列组合，得到一个一样优的解，这也就表示存在多组局部最小值。



这显然不符合凸优化问题的描述。



所以我们下结论：**深度学习问题是nonconvex的**

