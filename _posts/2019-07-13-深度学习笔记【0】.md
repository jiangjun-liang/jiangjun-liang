---
layout:     post                    # 使用的布局（不需要改）
title:     深度学习笔记【0】              # 标题 
subtitle:   从零开始搭建神经网络 #副标题
date:       2019-07-13         # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 神经网络
---

# Zero

让人兴奋的暑假开始了，今天开始我会定期更新一些有关于深度学习的博客，非常有幸活在这个时代，下面就让我开始介绍这篇文章的主要内容。


本文主要参考Victor Zhou先生的博文，在此向开源世界的牛人致敬(博客链接在本文最后)。


神经网络，本来是神经科学中的内容，但是受到启发的计算机科学家以此为框架，构建了具有很强大能力的神经网络，特别是当计算机的算力增强了以后，神经网络具有的强大学习能力，已经表现出十分巨大的潜能。


## 神经元

神经网络的最基本单元是神经元(Neuron)


在计算机科学中的神经元虽然是从真实的神经元中抽象而来，但是却更加简化。


![](https://victorzhou.com/perceptron-a74a19dc0599aae11df7493c718abaf9.svg)


首先神经元需要一个或多个输入，当然这些输入都是带有权值的，具体的权值一般我们刚开始是不清楚的，需要经过后期的训练才能得出，这也是深度学习需要**学习**的内容。


除了权重以外，神经元还应该具有的一个属性，叫阈值，很自然的，只有当神经元接受到的输入(刺激)超过阈值，神经元才应该被激活。


用python实现神经元如下


```python
class Neuron:
    def __init__(self,weights,bias):
        self.weights=weights
        self.bias=bias
```

## Hypothesis函数

如果输入的值比如$x$,是以简单的线性组合形式来输出的话，显然这样的模型过于简单了，一切都应该简单，但不应该过于简单。


我们需要一个预测的函数，将输入的$x$，映射为输出$y$


把这样的函数叫做Hypothesis函数，我们这里采用sigmoid函数，也就是：

$$
H(x)=\frac{1}{1+e^{-x}}
$$

这里的$x\in R^{n}$,是一个向量。


![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Sigmoid-function-2.svg/2000px-Sigmoid-function-2.svg.png)


sigmoid函数有一个很好的性质就是它的导数非常好计算:

$$
H'(x)=H(x)\times(1-H(x))
$$


用python实现如下：


```python
def sigmoid(x):
    return 1/(1+np.exp(-x))
def deriv_sigmoid(x):
    fx=sigmoid(x)
    return fx*(1-fx)
```

在这里，$H(x)$的输入就是$wx+b$,也就是说：

$$
y=H(wx+b)
$$

其中的$w$是神经元的权重，b是偏置值(起到设定阈值的作用)。


并且这样我们就可以写出神经元接受输入后输出预测值的函数。

```python
class Neuron:
    def __init__(self,weights,bias):
        self.weights=weights
        self.bias=bias
    
    def feedforward(self,inputs):
        total=np.dot(self.weights,inputs)+self.bias
        return sigmoid(total)
```

顺便提一下，sigmoid函数具有一个很好的性质，那就是它的值域属于$[0,1]$，这样就可以把输出的预测值看作是概率。

## 神经网络

单个神经元能做的事情非常有限，甚至连异或(XOR)这样的基本操作都无法做到,但是，当多个神经元组成神经网络的时候，就可以体现出神经网络具有的巨大潜能。


举一个例子来说明神经网络的作用

![](https://victorzhou.com/network-77ed172fdef54ca1ffcfb0bba27ba334.svg)

如上图所示，这个神经网络由三个神经元组成(蓝色)，具有两个输入，一个输出。


我们面对的是这样的一个问题


|  Name   | Weight | Height | Gender |
| :-----: | :----: | :----: | :----: |
|  Alice  |   -2   |   -1   |   1    |
|   Bob   |   25   |   6    |   0    |
| Charlie |   17   |   4    |   0    |
|  Diana  |  -15   |   -6   |   1    |


这里有一些数据，其中包括了四个小青年的身高体重(与平均值相比)，和性别。我们用0来表示男生，1来表示女生。

现在，*你能不能用神经网络来给我做一个简单的分类器，帮助我通过身高体重来判断这个人是男是女？*

这里的问题来了，我们的神经网络面对的输入是一组二维向量，需要输出的是介于0与1之间的值，我们可以认为这是判断输入的信息表示这个人是男生的概率。


### Loss Function

想要解决一个数学模型问题，首先很重要的一个工作就是如何衡量我这个数学模型的优劣。

这里介绍一种常用的衡量误差的函数(**Loss Function**)


**MSE** (mean squared error）
$$
MSE=\frac{1}{n}\sum_{i-1}^{n}(y_{true}-y_{pred})^2
$$
看到这个函数，是不是很像方差？


那么我们接下来要做的就是，通过修改神经网络中的权重和阈值，使得MSE尽可能的小，也就是一个优化问题。


首先用python实现MSE:

```python
def mse_loss(y_true,y_pred):
    return ((y_true-y_pred)**2).mean()
```

### 训练模型

![](https://victorzhou.com/network3-abd4b67172424f7e2c140b6759ee97bb.svg)

为了更加简单的说明，不妨假设此时只有一行数据用于训练。


回顾一下之前给出的sigmoid函数，我们可以写出：

$$
h_1=H(w_1\times weight+w_2\times height+b_1)\\
h_2=H(w_3\times weight+w_4\times height+b_2)
$$

这是中间的神经元的输出，同样的也可以得到

$$
y_{pred}=o_1=H(w_5\times h_1+w_6\times h_2+b_3)
$$

$o_1$正是这个神经网络在权值分别是$w_1,\dots,w_6$，阈值分别是$b_1,\dots b_3$,下的输出，而之前用于衡量神经网络优劣的loss function此时的取值为：

$$
L=\frac{1}{1}(y_{true}-y_{pred})^2
$$

不难看出，这里的$L$相当于$L(w_1,w_2,w_3,w_4,w_5,w_6,b_1,b_2,b_3)$，$y_{true}$是确定的常数

我们要做的就是对$L$做优化，改变参数值使得误差最小。

考虑到这里的$L$具有十分光滑的分析性质，我们对其做一下简单的分析，不妨考虑改变$w_1$，看看此时的$L$如何变化。

$$
\frac{\partial L}{\partial w_1}
$$

虽然此时的神经网络并不复杂，如果我们愿意，甚至可以直接的写出$L$关于$w_1$的表达式，但是当网络复杂起来以后，想要写出表达式就没那么容易了，不仅容易出错，效率也很低下。


我们考虑使用链式法则(**Chain Rule**)

$$
\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial y_{pred}}\times\frac{\partial y_{pred}}{\partial w_1}
$$

而$L$关于$y_{pred}$的导数确实可以一眼看出

$$
\frac{\partial L}{\partial y_{pred}}=-2(1-y_{pred})
$$

接下来就是去求$\frac{\partial y_{pred}}{\partial w_1}$,和之前的区别不大

$$
y_{pred}=o_1=H(w_5h_1+w_6h_2+b_3)
$$

接着用链式法则，有：

$$
\frac{\partial y_{pred}}{\partial w_1}=\frac{\partial y_{pred}}{\partial h_1}\times \frac{\partial h_1}{\partial w_1}
$$


$$
\frac{\partial y_{pred}}{\partial w_1}=w_5\times H'(w_5h_1+w_6h_2+b_3)
$$

这里的$H'$就不用继续算下去了，因为之前已经介绍过$H'=H\times （1-H）$

只要再算出$\frac{\partial h_1}{\partial w_1}$

$$
\frac{\partial h_1}{\partial w_1}=weight\times H'(weight\times w_1+height\times w_2+b_1)
$$

到这里，我们就已经可以得到需要的所有信息了。

$$
\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial y_{pred}}\times\frac{\partial y_{pred}}{\partial h_1}\times\frac{\partial h_1}{\partial w_1}
$$

看的出，这里计算偏导数的方法是基于链式法则，由后向前计算的，在深度学习领域有一个很专业的名词，称为**BP**(Backpropagation)算法。


其他的参数相关的偏导数，都是同样的原理，这里不赘述了。

下面带入一个真实的数据，来感受一下具体的过程。

取

- $w=1$
- $b=0$

| Name  | Weight($x_1$) | Height($x_2$) | Gender |
| :---: | :-----------: | :-----------: | :----: |
| Alice |      -2       |      -1       |   1    |

$$
h_1=H(w_1x_1+w_2x_2+b_1)=0.0474\\
h_2=0.0474\\
o_1=0.524\\
$$


而

$$
\frac{\partial L}{\partial y_{pred}}=-2(1-y_{pred})=-0.952\\
\frac{\partial y_{pred}}{\partial h_1}=w_5\times H'(w_5 h_1+w_6 h_2+b3)=0.249\\
\frac{\partial h_1}{\partial w_1}=x_1\times H'(w_1x_1+w_2x_2+b1)=-0.0904
$$

这样就算出：

$$
\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial y_{pred}}\times\frac{\partial y_{pred}}{\partial h_1}\times\frac{\partial h_1}{\partial w_1}=0.0214
$$

这说明当$w_1$增大一丢丢的时候，误差也会增大一丢丢。

那么知道变化趋势以后，怎么对参数进行修改呢？

### 梯度下降法(Gradient Descent)

熟悉数值分析的同学都对这个算法不陌生，也就是说：

$$
w_1\gets w_1 -\eta \frac{\partial L}{\partial w_1}
$$

其中，$\eta$称为学习速率，也称为下降步长，表示学习的“步子”。

一般取$\eta$为比较小的正数

具体的来说：

- 偏导数大于0，说明参数增大会带来误差，这说明参数需要减小，故而减去一个正数。
- 偏导数小于0，说明参数减小会带来误差，故而减去一个负数
- 偏导数等于0，这表明参数不需要改变，因为我们不知道它的具体信息，故而减去0

### 具体实现

有了以上的步骤，不难看出，只需要对每一个参数都进行梯度下降的修改，直到$L$为0，或是足够小的时候，我们停止算法。

python的全部实现如下：

```python
import numpy as np
import matplotlib.pyplot as plt
def sigmoid(x):
    return 1/(1+np.exp(-x))
def deriv_sigmoid(x):
    fx=sigmoid(x)
    return fx*(1-fx)
def mse_loss(y_true,y_pred):
    return ((y_true-y_pred)**2).mean()
time=np.array(range(0,100))
lost=np.linspace(0,100,100)

class Neuron:
    def __init__(self,weights,bias):
        self.weights=weights
        self.bias=bias
    
    def feedforward(self,inputs):
        total=np.dot(self.weights,inputs)+self.bias
        return sigmoid(total)

class NeuralNetwork:
    def __init__(self):
        self.w1=np.random.normal()
        self.w2=np.random.normal()
        self.w3=np.random.normal()
        self.w4=np.random.normal()
        self.w5=np.random.normal()
        self.w6=np.random.normal()

        self.b1=np.random.normal()
        self.b2=np.random.normal()
        self.b3=np.random.normal()
    def feedforward(self,x):
        h1=sigmoid(self.w1*x[0]+self.w2*x[1]+self.b1)
        h2=sigmoid(self.w3*x[0]+self.w4*x[1]+self.b2)
        o1=sigmoid(self.w5*h1+self.w6*h2+self.b3)
        return o1
    def train(self,data,all_y_trues):
        learn_rate=0.1
        epochs=1000
        for epoch in range(epochs):
            for x,y_true in zip(data,all_y_trues):
                sum_h1=self.w1*x[0]+self.w2*x[1]+self.b1
                h1=sigmoid(sum_h1)
                sum_h2=self.w3*x[0]+self.w4*x[1]+self.b2
                h2=sigmoid(sum_h2)
                sum_o1=self.w5*h1+self.w6*h2+self.b3
                o1=sigmoid(sum_o1)
                y_pred=o1

                #下面计算偏导数，用于改变权值
                d_L_d_ypred=-2*(y_true-y_pred)

                d_ypred_d_w5=h1*deriv_sigmoid(sum_o1)
                d_ypred_d_w6=h2*deriv_sigmoid(sum_o1)
                d_ypred_d_b3=deriv_sigmoid(sum_o1)

                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)
                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)

                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)
                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)
                d_h1_d_b1 = deriv_sigmoid(sum_h1)


                d_h2_d_w3=x[0]*deriv_sigmoid(sum_h2)
                d_h2_d_w4=x[1]*deriv_sigmoid(sum_h2)
                d_h2_d_b2=deriv_sigmoid(sum_h2)

                #更新权值和偏置值
                #h1
                self.w1-=learn_rate*d_L_d_ypred*d_ypred_d_h1* d_h1_d_w1
                self.w2-=learn_rate*d_L_d_ypred*d_ypred_d_h1* d_h1_d_w2
                self.b1-=learn_rate*d_L_d_ypred*d_ypred_d_h1* d_h1_d_b1
                #h2
                self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3
                self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4
                self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2

                # Neuron o1
                self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5
                self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6
                self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3

                #计算损失函数
            if epoch % 10 == 0:
                    y_preds = np.apply_along_axis(self.feedforward, 1, data)
                    loss = mse_loss(all_y_trues, y_preds)
                    lost[epoch//10] = loss
                    print("Epoch %d loss: %.3f" % (epoch, loss))


data=np.array([
    [-2,-1],
    [25,6],
    [17,4],
    [-15,-6]
])
all_y_trues=np.array([
    1,
    0,
    0,
    1,
])
network=NeuralNetwork()
network.train(data,all_y_trues)
plt.plot(time,lost)
plt.show()
```

![](https://victorzhou.com/static/99e7886af56d6f41b484d17a52f9241b/cdbc6/loss.png)

训练误差的趋势像上图所示。

## 参考来源

<https://victorzhou.com/blog/intro-to-neural-networks/>