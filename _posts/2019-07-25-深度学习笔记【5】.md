---
layout:     post                    # 使用的布局（不需要改）
title:      深度学习笔记【5】               # 标题 
subtitle:    神经网络训练【SGDM】	#副标题
date:       2019-07-25            # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 神经网络
---

# Five

熟悉机器学习的人肯定都听说过**SGD**(Stochastic Gradient Descent)的大名,也就是说，每一次更新参数，都选择的是当前位置的梯度方向的反方向。简单的用代码表示可以写成：

```python
x+= -learning_rate*dx
```

其中的$dx$的方向，就是当前位置的梯度方向。

## SGD的不足

但是，当我们训练的神经网络的维度变得很高的时候，**SGD**往往会经过一段十分曲折的山路才有可能到达比较好的低点，甚至有些时候，停在稍微平坦一些的半山腰就不走了。



![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Saddle_point.png/220px-Saddle_point.png)



上图展示的是一个鞍点的例子，可以看到，在鞍点附近的梯度为0，但是鞍点处并不是最小值，也不是最大值。



而我们的SGD,因为需要梯度来更新方向，所以如果遇到一个梯度为0，或者说梯度十分接近0的位置，它就会天真的认为这就是我们需要的好位置，乖乖的不动弹了。



## 物理学上的启示—Momentum

试想你是一个从山上往下走的小球(或许说滚下去更合适~)，你除了会自然的沿着最陡峭的方向走以外，你还会受到重力场的作用，不自觉的变化你自己的动量。



如果说在你的前方有一个暂时的小小的坑，虽然你沿着既定方向会陷进去，但是由于你具有一定的动量，你凭借着动量，还是可以从这个小小的坑中出来，继续向下，直到你到达了一段非常不错的山谷。在你的附近，所有的方向都会让你的高度增加，而你此时也由于速度不是非常够，没办法再冲出这个小山谷了，得了，歇着吧。



以上就是从物理学中获得的启示，这暗示我们在更新我们的参数的时候，不应该只考虑当下位置的梯度，也应该结合之前我们得到的速度，综合的给参数做更新。

![](https://ae01.alicdn.com/kf/Hd3f6c7ff6d5946979d6be6e01de5336ew.gif)

写成代码的形式，大概如下：

```python
v = rho * v - learning_rate * dx#速度
x += v#更新参数
```

其中的$v$就是我们之前提到的与动量有关的项，一般初始化为0，这是一个**超参数**，而每一次向下走一步，都需要综合考虑之前走过的路的积累速度和当前的梯度，这显然要更加合理一些。



>有些遗憾的是，虽然方法的名字叫SGD with Momentum，但是实际中，我们写成代码的形式更像是摩擦力 :) ,这并不影响SGD with Momentum的优良表现。



## 其他的一些变种–**Nesterov Momentum**

Nesterov Momentum的基本思想是：如果我先沿着之前累计的方向走，再综合考虑走到的地方的梯度，来更新我的参数，会不会有更好的结果？



![](http://cs231n.github.io/assets/nn3/nesterov.jpeg)



上图说明了两种Momentum的不同，但是可以看出来，思想都是类似的。



而Nesterov Momentum更多的有一种**向前看**的理念。

如果我们用代码来表示更新的流程的话：



```python
x_ahead = x + rho * v
v = rho * v - learning_rate * dx_ahead
x += v
```



## 参考

[CS231n](<http://cs231n.github.io/neural-networks-3/#sgd>)

