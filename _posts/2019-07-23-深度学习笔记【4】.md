---
layout:     post                    # 使用的布局（不需要改）
title:      深度学习笔记【4】               # 标题 
subtitle:   神经网络的训练【激活函数篇】 #副标题
date:       2019-07-23            # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 神经网络
---

# Four

神经网络的强大功能我们在之前已经介绍过了，但是一个好的神经网络并不是天上掉下来的，所谓的深度学习，除了神经网络的结构以外，很重要的一步就是**神经网络的训练**。而神经网络的训练，这一个问题，也是十分活跃的领域，有许多大牛都在这个领域发表很多很牛的论文，日后我会一一拜读，但是目前，先让我们熟悉一下，在神经网络训练中常见的激活函数吧。



## 常见的激活函数

### Sigmoid

![](https://ae01.alicdn.com/kf/Ha3b4443192034779b12602ecadbd574fv.png)



初次入门机器学习的同学可能对于**Sigmoid**函数非常熟悉，这个函数的解析形式是这样的：


$$
\sigma(x)=\frac{1}{1+e^{-x}}
$$


从上面的图中我们可以很容易的看出$\sigma(x)$具有以下几个优点：

- 可以将$R$上的数，映射到$(0,1)$区间上，可以用于表示概率的问题。
- 导数容易计算，方便使用BP算法进行梯度传播



但是，我们使用的很顺手的**Sigmoid**函数，也存在几个很大的缺点：

- **容易饱和**，比如，当输入的$x$是一个绝对值稍微大一些的数，此时的$\sigma(x)$就十分接近其两端，也就是0和1。这个缺点就会导致两个可能相差很大的数，比如10，100，但是其经过$\sigma(x)$后的值几乎相同。

- **梯度弥散**，这个问题要比上一个问题来得更加严重，我们训练神经网络的最主要手段都是基于**梯度**的，而$\sigma(x)$的导数，当输入的值绝对值稍微大一些，得到的导数都趋向于0，这样就会导致后向传播梯度的时候，得到的梯度十分接近0，也就是训练不起来了，神经网络的训练到此就几乎**停滞不前**。并且这样的问题存在，就使得我们在初始化参数的时候需要十分的小心，不然很有可能在训练的刚开始处，梯度就弥散了，训练结束，惨不忍睹。

- **非零中心**(not zero-centered),这是指，$\sigma(x)$的输出值，平均而言并不是以0为中心的，这样会带来一个问题，由于$\sigma(x)$的输出与导数都是非负的，这将导致在反向传播梯度的时候：

  

  
  $$
  e.g. \quad x>0\quad f=w^Tx+b
  $$
  

  对$w$的梯度要么都是正数，要么都是负数(符号取决于$f$),这会导致在更新参数的时候出现锯齿现象(zig-zagging)，这将使得收敛速度变得很慢。

  

  当然，这样的问题和前面的梯度弥散比起来，并不算十分严重。

  

  

### Tanh

![](https://ae01.alicdn.com/kf/HTB1D45SbkT2gK0jSZPc763KkpXag.png)

事实上：


$$
tanh(x)=2\sigma(2x)-1
$$


所以$tanh(x)$的性质与$\sigma(x)$十分接近，除了其是**零均值**的，其他的缺点与优点都与$\sigma(x)$相似。



### ReLU(Rectified Linear Unit)

![](https://ae01.alicdn.com/kf/HTB1GliRboT1gK0jSZFh761AtVXaw.png)

ReLU函数是近来(2012-今),十分流行的一个激活函数，其解析形式为:


$$
f(x)=max(0,x)
$$


从函数图像上看，这个函数十分的简单，但是在实际使用中，我们发现它的效果非常好，相比之前的$\sigma(x) , tanh(x)$，它的效果要好很多。



以下是ReLU函数的一些优点：

- 在实际中，发现其十分容易加速随机梯度下降法的收敛，相较于之前的激活函数。(有人认为是由于它的不易饱和性质和它的线性性质)
- 计算十分简单，不需要进行复杂的指数运算

当然，它也有缺点：

- 数据幅度会不断增加，因为它不像$tanh(x),\sigma(x)$一样具有压缩数据的功能，故而容易出现溢出的情况。

- 容易造成**神经元死亡**，这听上去好像很吓人，但是实际上是指，由于ReLU的输出和导数，很有可能因为输入的关系变成0，这样就可能导致在接下来的训练中，使用ReLU作为激活函数的神经元再也不会更新(通过实践，我们发现这往往与学习率相关，如果学习率过大，可能会造成很多问题，这方面的问题，读者可以自行实践。)，当然，通过减小学习率，我们可以减轻这个问题。

> 插句题外话，据我所知，人在使用大脑的时候，只有很少部分的神经元处于激活状态，大部分的神经元都像是睡着了一样，这种特性和ReLU带来的特性有部分相似之处，所以我个人认为，这可能算不上太大的缺点，反而有可能是其效果好的某些隐藏原因？



### Leaky ReLU

![](https://ae01.alicdn.com/kf/HTB1Vm9Tbhn1gK0jSZKP760vUXXad.png)

Leaky ReLU与ReLU十分相似，唯一的区别就在于其的解析表达为：


$$
f(x)=max(\alpha x,x)
$$
一般$\alpha $取小于0.1的值，上图为了看得更加明显，取$\alpha=0.1$



Leaky ReLU解决了ReLU的**神经元死亡**的问题，但是：



**在实践中，效果并不比ReLU来得好**



## Summary

**TLDR:** 

> “*What neuron type should I use?*” Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.

一言以蔽之：ReLU大法好



## 参考

本文是在博主学习Stanford的CS231n这门课后的一些总结，其中有我自己的一些看法和经验，也大量参考了CS231n的内容，相关的[官方笔记在这里](<http://cs231n.github.io/neural-networks-1/>),在这里感谢公开高质量教育课程的世界名校！

